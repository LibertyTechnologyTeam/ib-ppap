name: Project Influence  Schedule Save Crawler data
on:
  workflow_dispatch:
    inputs:
      reason:
        description: "Fill reason here!!!!!!"
        required: false
  repository_dispatch:
    types: [schedule_save_crawler_data]
env:
  API_INFLUENCERS_URL: ${{ github.event.client_payload.api_url}}
  GH_PAT: ${{ secrets.GH_PAT }}
  SIZE: ${{ github.event.client_payload.size }}
  PACKAGE_PATH: ${{ github.event.client_payload.package_path }}
  SIZE_PER_MATRIX: ${{ github.event.client_payload.size_per_matrix }}
jobs:
  get-profile-data:
    name: Get Profile Input Data
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.generate-cache-key.outputs.key }}
      matrix: ${{ steps.calculate-matrix.outputs.matrix }}
    steps:
      - name: Clone repository
        uses: actions/checkout@v3
        with:
          repository: "InfluenceBooking/crawler"
          token: ${{ secrets.GH_PAT }}
          ref: "main"
      # Generate a unique cache key
      - name: Generate cache key
        id: generate-cache-key
        run: |
          echo "key=${{ runner.os }}-crawler-data-${{ github.sha }}" >> $GITHUB_OUTPUT

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20.10.0"
      - name: Install dependencies
        run: |
          npm install
      - name: Run script
        id: script
        run: |
          echo "API_INFLUENCERS_URL=${{ env.API_INFLUENCERS_URL }}" >> .env
          echo 'import { writeInputData } from "./src/write-input-data.js"; writeInputData(${{ env.SIZE }}, ${{ env.PACKAGE_PATH }});' > ./main.js
          echo "length=$(node --no-warnings ./main.js)" >> $GITHUB_OUTPUT

      - name: Calculate matrix
        id: calculate-matrix
        run: |
          matrix_size=$(echo "scale=0; (${{steps.script.outputs.length}} / ${{env.SIZE_PER_MATRIX}})" | bc)
          matrix="["
          for ((i=0; i<$matrix_size; i++)); do matrix+="\"$i\", "; done
          matrix+="]"
          echo "matrix=$matrix" >> $GITHUB_OUTPUT
      - name: Cache crawler data
        uses: actions/cache@v3
        with:
          path: |
            input_influencers_all.json
          key: ${{ steps.generate-cache-key.outputs.key }}
          restore-keys: ${{ runner.os }}-crawler-data-

  crawl-profiles:
    needs: get-profile-data
    runs-on: ubuntu-latest
    name: Crawl Social Profiles
    strategy:
      matrix:
        index: ${{ fromJson(needs.get-profile-data.outputs.matrix) }}
      # Allow all matrix jobs to continue even if one fails
      fail-fast: false
    steps:
      - name: Use matrix
        run: |
          echo "Processing index: ${{ matrix.index }}"
      - name: Clone repository
        uses: actions/checkout@v3
        with:
          repository: "InfluenceBooking/crawler"
          token: ${{ secrets.GH_PAT }}
          ref: "main"
      - name: Get cache key
        id: get-cache-key
        run: |
          echo "cache-key=${{ needs.get-profile-data.outputs.cache-key }}" >> $GITHUB_OUTPUT

      - name: Restore crawler data
        uses: actions/cache@v3
        with:
          path: |
            input_influencers_all.json
          key: ${{ needs.get-profile-data.outputs.cache-key }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20.10.0"

      - name: Install dependencies
        run: npm install

      - name: Run crawl profile
        run: |
          echo "platform=${{ matrix.index }}"
          echo 'import { writeOutputByIndex } from "./src/write-output-data.js"; writeOutputByIndex(${{ matrix.index }}, ${{ env.SIZE_PER_MATRIX }});' > ./main.js
          echo "API_INFLUENCERS_URL=${{ env.API_INFLUENCERS_URL }}" >> .env
          node --no-warnings main.js
